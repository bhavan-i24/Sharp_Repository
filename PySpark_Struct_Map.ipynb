{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27cc1c75-8143-4f7a-962e-15a8e0e500ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d137cbb-a8ba-4f38-a35d-85ce32fa74dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+\n|pickup_info                 |\n+----------------------------+\n|{10103, 2016-02-13 21:47:53}|\n|{10023, 2016-02-13 18:29:09}|\n|{10001, 2016-02-06 19:40:58}|\n|{10044, 2016-02-12 19:06:43}|\n|{10199, 2016-02-23 10:27:56}|\n+----------------------------+\nonly showing top 5 rows\nroot\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n |-- pickup_info: struct (nullable = false)\n |    |-- pickup_zip: integer (nullable = true)\n |    |-- tpep_pickup_datetime: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"pickup_info\",\n",
    "    struct(\n",
    "        col(\"pickup_zip\"),\n",
    "        col(\"tpep_pickup_datetime\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df1.select(\"pickup_info\").show(5, truncate=False)\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "915f3d93-8104-4eb3-a0bb-655990951709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n|pickup_zip|tpep_pickup_datetime|\n+----------+--------------------+\n|     10103| 2016-02-13 21:47:53|\n|     10023| 2016-02-13 18:29:09|\n|     10001| 2016-02-06 19:40:58|\n|     10044| 2016-02-12 19:06:43|\n|     10199| 2016-02-23 10:27:56|\n+----------+--------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    col(\"pickup_info.pickup_zip\"),\n",
    "    col(\"pickup_info.tpep_pickup_datetime\")\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87aadeee-6e5f-4910-b47e-4f5cf5705a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+\n|  zip|        pickup_time|\n+-----+-------------------+\n|10103|2016-02-13 21:47:53|\n|10023|2016-02-13 18:29:09|\n|10001|2016-02-06 19:40:58|\n|10044|2016-02-12 19:06:43|\n|10199|2016-02-23 10:27:56|\n+-----+-------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df1.select(\n",
    "    col(\"pickup_info\").getField(\"pickup_zip\").alias(\"zip\"),\n",
    "    col(\"pickup_info\").getField(\"tpep_pickup_datetime\").alias(\"pickup_time\")\n",
    ").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ab9e17-5b2b-4433-9b81-f852c17747f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n|pickup_info                      |\n+---------------------------------+\n|{10103, 2016-02-13 21:47:53, NYC}|\n|{10023, 2016-02-13 18:29:09, NYC}|\n|{10001, 2016-02-06 19:40:58, NYC}|\n|{10044, 2016-02-12 19:06:43, NYC}|\n|{10199, 2016-02-23 10:27:56, NYC}|\n+---------------------------------+\nonly showing top 5 rows\nroot\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n |-- pickup_info: struct (nullable = false)\n |    |-- pickup_zip: integer (nullable = true)\n |    |-- tpep_pickup_datetime: timestamp (nullable = true)\n |    |-- city: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df2 = df1.withColumn(\n",
    "    \"pickup_info\",\n",
    "    col(\"pickup_info\").withField(\"city\", lit(\"NYC\"))\n",
    ")\n",
    "\n",
    "df2.select(\"pickup_info\").show(5, truncate=False)\n",
    "df2.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aa51f62-ba5e-40d9-87b4-52a5df4f020d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n|pickup_info                      |\n+---------------------------------+\n|{10103, 2016-02-13 21:47:53, NYC}|\n|{10023, 2016-02-13 18:29:09, NYC}|\n|{10001, 2016-02-06 19:40:58, NYC}|\n|{10044, 2016-02-12 19:06:43, NYC}|\n|{10199, 2016-02-23 10:27:56, NYC}|\n+---------------------------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.withColumn(\n",
    "    \"pickup_info\",\n",
    "    col(\"pickup_info\").withField(\n",
    "        \"pickup_zip\",\n",
    "        col(\"pickup_info.pickup_zip\").cast(\"int\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df3.select(\"pickup_info\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1b7b780-7501-4ef6-93cd-80cfc8fc5d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n|pickup_info                      |\n+---------------------------------+\n|{10103, 2016-02-13 21:47:53, NYC}|\n|{10023, 2016-02-13 18:29:09, NYC}|\n|{10001, 2016-02-06 19:40:58, NYC}|\n|{10044, 2016-02-12 19:06:43, NYC}|\n|{10199, 2016-02-23 10:27:56, NYC}|\n+---------------------------------+\nonly showing top 5 rows\nroot\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n |-- pickup_info: struct (nullable = false)\n |    |-- zip: integer (nullable = true)\n |    |-- pickup_time: timestamp (nullable = true)\n |    |-- city: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.withColumn(\n",
    "    \"pickup_info\",\n",
    "    struct(\n",
    "        col(\"pickup_info.pickup_zip\").alias(\"zip\"),\n",
    "        col(\"pickup_info.tpep_pickup_datetime\").alias(\"pickup_time\"),\n",
    "        col(\"pickup_info.city\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df4.select(\"pickup_info\").show(5, truncate=False)\n",
    "df4.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ac1b53-7350-4ee4-b970-1ea188545bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n\n+--------------------+---------------------+-------------+-----------+----------+-----------+\n|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|fare_amount|pickup_zip|dropoff_zip|\n+--------------------+---------------------+-------------+-----------+----------+-----------+\n| 2016-02-13 21:47:53|  2016-02-13 21:57:15|          1.4|        8.0|     10103|      10110|\n| 2016-02-13 18:29:09|  2016-02-13 18:37:23|         1.31|        7.5|     10023|      10023|\n| 2016-02-06 19:40:58|  2016-02-06 19:52:32|          1.8|        9.5|     10001|      10018|\n| 2016-02-12 19:06:43|  2016-02-12 19:20:54|          2.3|       11.5|     10044|      10111|\n| 2016-02-23 10:27:56|  2016-02-23 10:58:33|          2.6|       18.5|     10199|      10022|\n+--------------------+---------------------+-------------+-----------+----------+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.table(\"samples.nyctaxi.trips\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38076736-56cf-436e-9b8b-6d30a96acc73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n|trip_metrics                   |\n+-------------------------------+\n|{fare -> 8.0, distance -> 1.4} |\n|{fare -> 7.5, distance -> 1.31}|\n|{fare -> 9.5, distance -> 1.8} |\n|{fare -> 11.5, distance -> 2.3}|\n|{fare -> 18.5, distance -> 2.6}|\n+-------------------------------+\nonly showing top 5 rows\nroot\n |-- tpep_pickup_datetime: timestamp (nullable = true)\n |-- tpep_dropoff_datetime: timestamp (nullable = true)\n |-- trip_distance: double (nullable = true)\n |-- fare_amount: double (nullable = true)\n |-- pickup_zip: integer (nullable = true)\n |-- dropoff_zip: integer (nullable = true)\n |-- trip_metrics: map (nullable = false)\n |    |-- key: string\n |    |-- value: double (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map, col, lit\n",
    "\n",
    "df1 = df.withColumn(\n",
    "    \"trip_metrics\",\n",
    "    create_map(\n",
    "        lit(\"fare\"), col(\"fare_amount\"),\n",
    "        lit(\"distance\"), col(\"trip_distance\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df1.select(\"trip_metrics\").show(5, truncate=False)\n",
    "df1.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9af8d5b3-36a5-4e2e-bde2-41656ed11144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|            keys|\n+----------------+\n|[fare, distance]|\n|[fare, distance]|\n|[fare, distance]|\n|[fare, distance]|\n|[fare, distance]|\n+----------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df1.select(map_keys(col(\"trip_metrics\")).alias(\"keys\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bf7a79a-057a-4430-9220-a180bc566ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n|     values|\n+-----------+\n| [8.0, 1.4]|\n|[7.5, 1.31]|\n| [9.5, 1.8]|\n|[11.5, 2.3]|\n|[18.5, 2.6]|\n+-----------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df1.select(map_values(col(\"trip_metrics\")).alias(\"values\")).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6490bc27-084a-47df-b7dd-96f7ebadabda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark_Struct_Map",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}